{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in /opt/anaconda3/envs/MLE/lib/python3.10/site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pCR (outcome)</th>\n",
       "      <th>RelapseFreeSurvival (outcome)</th>\n",
       "      <th>Age</th>\n",
       "      <th>ER</th>\n",
       "      <th>PgR</th>\n",
       "      <th>HER2</th>\n",
       "      <th>TrippleNegative</th>\n",
       "      <th>ChemoGrade</th>\n",
       "      <th>Proliferation</th>\n",
       "      <th>HistologyType</th>\n",
       "      <th>...</th>\n",
       "      <th>original_glszm_SmallAreaHighGrayLevelEmphasis</th>\n",
       "      <th>original_glszm_SmallAreaLowGrayLevelEmphasis</th>\n",
       "      <th>original_glszm_ZoneEntropy</th>\n",
       "      <th>original_glszm_ZonePercentage</th>\n",
       "      <th>original_glszm_ZoneVariance</th>\n",
       "      <th>original_ngtdm_Busyness</th>\n",
       "      <th>original_ngtdm_Coarseness</th>\n",
       "      <th>original_ngtdm_Complexity</th>\n",
       "      <th>original_ngtdm_Contrast</th>\n",
       "      <th>original_ngtdm_Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>144.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517172</td>\n",
       "      <td>0.375126</td>\n",
       "      <td>3.325332</td>\n",
       "      <td>0.002314</td>\n",
       "      <td>3880771.500</td>\n",
       "      <td>473.464852</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.182615</td>\n",
       "      <td>0.030508</td>\n",
       "      <td>0.000758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444391</td>\n",
       "      <td>0.444391</td>\n",
       "      <td>3.032144</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>2372009.744</td>\n",
       "      <td>59.459710</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>0.032012</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.003685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>135.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534549</td>\n",
       "      <td>0.534549</td>\n",
       "      <td>2.485848</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>1540027.421</td>\n",
       "      <td>33.935384</td>\n",
       "      <td>0.007584</td>\n",
       "      <td>0.024062</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.006447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506185</td>\n",
       "      <td>0.506185</td>\n",
       "      <td>2.606255</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>6936740.794</td>\n",
       "      <td>46.859265</td>\n",
       "      <td>0.005424</td>\n",
       "      <td>0.013707</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.004543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462282</td>\n",
       "      <td>0.462282</td>\n",
       "      <td>2.809279</td>\n",
       "      <td>0.006521</td>\n",
       "      <td>1265399.054</td>\n",
       "      <td>39.621023</td>\n",
       "      <td>0.006585</td>\n",
       "      <td>0.034148</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.005626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pCR (outcome)  RelapseFreeSurvival (outcome)   Age  ER  PgR  HER2  \\\n",
       "0              1                          144.0  41.0   0    0     0   \n",
       "1              0                          142.0  39.0   1    1     0   \n",
       "2              1                          135.0  31.0   0    0     0   \n",
       "3              0                           12.0  35.0   0    0     0   \n",
       "4              0                          109.0  61.0   1    0     0   \n",
       "\n",
       "   TrippleNegative  ChemoGrade  Proliferation  HistologyType  ...  \\\n",
       "0                1           3              3              1  ...   \n",
       "1                0           3              3              1  ...   \n",
       "2                1           2              1              1  ...   \n",
       "3                1           3              3              1  ...   \n",
       "4                0           2              1              1  ...   \n",
       "\n",
       "   original_glszm_SmallAreaHighGrayLevelEmphasis  \\\n",
       "0                                       0.517172   \n",
       "1                                       0.444391   \n",
       "2                                       0.534549   \n",
       "3                                       0.506185   \n",
       "4                                       0.462282   \n",
       "\n",
       "   original_glszm_SmallAreaLowGrayLevelEmphasis  original_glszm_ZoneEntropy  \\\n",
       "0                                      0.375126                    3.325332   \n",
       "1                                      0.444391                    3.032144   \n",
       "2                                      0.534549                    2.485848   \n",
       "3                                      0.506185                    2.606255   \n",
       "4                                      0.462282                    2.809279   \n",
       "\n",
       "   original_glszm_ZonePercentage  original_glszm_ZoneVariance  \\\n",
       "0                       0.002314                  3880771.500   \n",
       "1                       0.005612                  2372009.744   \n",
       "2                       0.006752                  1540027.421   \n",
       "3                       0.003755                  6936740.794   \n",
       "4                       0.006521                  1265399.054   \n",
       "\n",
       "   original_ngtdm_Busyness  original_ngtdm_Coarseness  \\\n",
       "0               473.464852                   0.000768   \n",
       "1                59.459710                   0.004383   \n",
       "2                33.935384                   0.007584   \n",
       "3                46.859265                   0.005424   \n",
       "4                39.621023                   0.006585   \n",
       "\n",
       "   original_ngtdm_Complexity  original_ngtdm_Contrast  original_ngtdm_Strength  \n",
       "0                   0.182615                 0.030508                 0.000758  \n",
       "1                   0.032012                 0.001006                 0.003685  \n",
       "2                   0.024062                 0.000529                 0.006447  \n",
       "3                   0.013707                 0.000178                 0.004543  \n",
       "4                   0.034148                 0.001083                 0.005626  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Install xlrd package\n",
    "%pip install xlrd\n",
    "\n",
    "all_df = pd.read_excel('TrainDataset2024.xls', index_col=False)\n",
    "all_df.drop('ID', axis=1, inplace=True)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Replace missing values with median of the column\n",
    "imputer = SimpleImputer(strategy=\"median\", missing_values=999)\n",
    "all_df[:] = imputer.fit_transform(all_df)\n",
    "\n",
    "# classification target\n",
    "clf_y = all_df['pCR (outcome)']\n",
    "# regression target\n",
    "rgr_y = all_df['RelapseFreeSurvival (outcome)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier removal approach by:\n",
    "# Thanaki, Jalaj. Machine Learning Solutions : Expert Techniques to Tackle Complex Machine Learning Problems Using Python, Packt Publishing, Limited, 2018. \n",
    "# ProQuest Ebook Central, Available at: http://ebookcentral.proquest.com/lib/nottingham/detail.action?docID=5379696.\n",
    "\n",
    "# Outlier detection using the following methods:\n",
    "# 1. Percentile based outlier detection\n",
    "# 2. MAD (median absolute deviation) based outlier detection\n",
    "# 3. Standard deviation based outlier detection\n",
    "\n",
    "\"\"\" \n",
    "    Get all the data points that lie under the percentile range from 2.5 to 97.5\n",
    "\"\"\"\n",
    "def percentile_based_outlier(data, threshold=95):\n",
    "    diff = (100 - threshold) / 2.0\n",
    "    minval, maxval = np.percentile(data, [diff, 100 - diff])\n",
    "    return (data < minval) | (data > maxval)\n",
    "\n",
    "\"\"\"\n",
    "    Get all the data points that lie under a threshold of 3.5 using modified Z-score (based on the median absolute deviation)\n",
    "\"\"\"\n",
    "def mad_based_outlier(points, threshold=3.5):\n",
    "    points = np.array(points)\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:, None]\n",
    "    median_y = np.median(points)\n",
    "    median_absolute_deviation_y = np.median([np.abs(y - median_y) for y in points])\n",
    "    # Small constant added to avoid division by zero\n",
    "    modified_z_scores = [0.6745 * (y - median_y) / (median_absolute_deviation_y + 1e-6) for y in points]\n",
    "\n",
    "    return np.abs(modified_z_scores) > threshold\n",
    "\n",
    "\"\"\"\n",
    "    Get all the data points that lie under a threshold of 3 using standard deviation\n",
    "\"\"\"\n",
    "def std_div(data, threshold=3):\n",
    "    std = data.std()\n",
    "    mean = data.mean()\n",
    "    isOutlier = []\n",
    "    for val in data:\n",
    "        if abs(val - mean)/std > threshold:\n",
    "            isOutlier.append(True)\n",
    "        else:\n",
    "            isOutlier.append(False)\n",
    "    return isOutlier\n",
    "\n",
    "\"\"\"\n",
    "    Perform an outlier voting system to determine if a data point is an outlier. \n",
    "    If two of the three methods agree that a data point is an outlier, then it is removed.\n",
    "\"\"\"\n",
    "def outlierVote(data):\n",
    "    x = percentile_based_outlier(data)\n",
    "    y = mad_based_outlier(data)\n",
    "    z = std_div(data)\n",
    "    temp = list(zip(x, y, z))\n",
    "    final = []\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i].count(False) >= 2:\n",
    "            final.append(False)\n",
    "        else:\n",
    "            final.append(True)\n",
    "    return final\n",
    "\n",
    "def removeOutliers(data):\n",
    "    # Remove outliers from the dataframe\n",
    "    for column in data.columns:\n",
    "        outliers = outlierVote(all_df[column])\n",
    "        # Calculate Non-Outlier Maximum using the outliers list\n",
    "        non_outlier_max = all_df.loc[~np.array(outliers), column].max()\n",
    "        # Replace outliers with the maximum non-outlier value\n",
    "        data.loc[outliers, column] = non_outlier_max\n",
    "\n",
    "# Remove outliers, assign modified features to X and drop the outcome columns\n",
    "removeOutliers(all_df)\n",
    "X = all_df.drop(['pCR (outcome)', 'RelapseFreeSurvival (outcome)'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalisation/Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardise features by removing the mean and scaling to unit variance.\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\"\"\" # Select required features (ER, HER2 and Gene)\n",
    "required_features = Xs[:, [1, 3, 10]]\n",
    "\n",
    "# Apply LDA to all features except required ones\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "Xs_lda = lda.fit_transform(np.delete(Xs, [1, 3, 10], axis=1), clf_y)\n",
    "\n",
    "# Combine required features with LDA transformed features\n",
    "Xs = np.hstack((required_features, Xs_lda)) \"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Normalise the features to use zero mean normalisation\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "\n",
    "mandatory_features = ['ER', 'HER2', 'Gene']\n",
    "mandatory_features_indices = [1,3,10]\n",
    "required_features = Xs[:, mandatory_features_indices]\n",
    "\n",
    "X_non_required = np.delete(Xs, mandatory_features_indices, axis=1)\n",
    "\n",
    "X_gaussian = []\n",
    "X_non_gaussian = []\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Check if the features are Gaussian\n",
    "for column in range(X_non_required.shape[1]):\n",
    "    feature_data = X_non_required[:, column]\n",
    "\n",
    "    normal_count = 0\n",
    "\n",
    "    stat, p = stats.shapiro(feature_data)\n",
    "    # interpret\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        normal_count += 1\n",
    "\n",
    "    stat, p = stats.normaltest(feature_data)\n",
    "    # interpret\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        normal_count += 1\n",
    "\n",
    "    result = stats.anderson(feature_data)\n",
    "    p = 0\n",
    "    for i in range(len(result.critical_values)):\n",
    "        sl, cv = result.significance_level[i], result.critical_values[i]\n",
    "        if result.statistic < result.critical_values[i]:\n",
    "            normal_count += 1\n",
    "        \n",
    "    if normal_count >= 2:\n",
    "        X_gaussian.append(column)\n",
    "    else:\n",
    "        X_non_gaussian.append(column)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_gaussian = X_non_required[:, X_gaussian]\n",
    "X_non_gaussian = X_non_required[:, X_non_gaussian]\n",
    "\n",
    "# Manifold Learning\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=1) \n",
    "Xs_tsne = tsne.fit_transform(X_non_gaussian)\n",
    "\n",
    "\"\"\" # Apply LDA to all features except required ones\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "Xs_lda = lda.fit_transform(X_gaussian, clf_y) \"\"\"\n",
    "\n",
    "# Apply LDA to all features except required ones\n",
    "pca = PCA(n_components=1)\n",
    "Xs_pca = pca.fit_transform(X_gaussian, clf_y)\n",
    "\n",
    "# Combine required features with LDA transformed features\n",
    "Xs = np.hstack((required_features, Xs_pca, Xs_tsne))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.790 (0.046)\n",
      "Best Model: SVC(C=10, gamma=0.01, kernel='poly', random_state=2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89       316\n",
      "           1       0.77      0.12      0.21        84\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.79      0.55      0.55       400\n",
      "weighted avg       0.80      0.81      0.75       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy._core.fromnumeric import mean\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy import std\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# this nested cross-validation is used to tune the hyperparameters of the model\n",
    "# the code was adapted from: \n",
    "# Brownlee, J. (2018). Nested Cross-Validation for Machine Learning with Python. [online] Machine Learning Mastery.\n",
    "# Available at: https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/ [Accessed 5 Dec. 2024]\n",
    "\n",
    "# configure the cross-validation procedure for the inner loop\n",
    "cv_inner = KFold(n_splits=5, shuffle=True, random_state=2)\n",
    "\n",
    "# define the classifier\n",
    "classifier = SVC(random_state=2)\n",
    "\n",
    "# define search space of hyperparameters\n",
    "space = dict()\n",
    "space['C'] = [0.1, 1, 10]\n",
    "space['gamma'] = [0.01, 0.1]\n",
    "space['kernel'] = [\"rbf\", \"linear\", \"poly\"]\n",
    "\n",
    "# define GridSearch to search for the best hyperparameters\n",
    "search = GridSearchCV(classifier, space, scoring='accuracy', n_jobs=1, cv=cv_inner, refit=True)\n",
    "\n",
    "# configure the cross-validation procedure for the outer loop\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=2)\n",
    "\n",
    "# execute the nested cross-validation\n",
    "scores = cross_val_score(search, Xs, clf_y, scoring='accuracy', cv=cv_outer, n_jobs=-1)\n",
    "\n",
    "# fit the search on the whole dataset to get the best model\n",
    "search.fit(Xs, clf_y)\n",
    "model = search.best_estimator_\n",
    "\n",
    "# report performance and best model configuration\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "print('Best Model:', model)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print f1-score, precision, recall and support\n",
    "y_pred = model.predict(Xs)\n",
    "print(classification_report(clf_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.762 (0.026)\n",
      "Best Model: RandomForestClassifier(max_features=4, random_state=1)\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Model:\u001b[39m\u001b[38;5;124m'\u001b[39m, best_model)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Print f1-score, precision, recall and support\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(clf_y, y_pred))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MLE/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:904\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    884\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 904\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MLE/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:944\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    923\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;124;03m    Predict class probabilities for X.\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03m        classes corresponds to that in the attribute :term:`classes_`.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 944\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m    946\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MLE/lib/python3.10/site-packages/sklearn/utils/validation.py:1661\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "from numpy import mean, std\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# configure the cross-validation procedure\n",
    "cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# define the model\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "# define search space\n",
    "space = dict()\n",
    "space['n_estimators'] = [10, 100, 500]\n",
    "space['max_features'] = [2, 4, 6]\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=1, cv=cv_inner, refit=True)\n",
    "# configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# execute the nested cross-validation\n",
    "scores = cross_val_score(search, Xs, clf_y, scoring='accuracy', cv=cv_outer, n_jobs=-1)\n",
    "# fit the search on the whole dataset to get the best model\n",
    "search.fit(Xs, clf_y)\n",
    "best_model = search.best_estimator_\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "print('Best Model:', best_model)\n",
    "\n",
    "# Print f1-score, precision, recall and support\n",
    "y_pred = model.predict(Xs)\n",
    "print(classification_report(clf_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.792 (0.024)\n",
      "Best Model: LogisticRegression(C=10, random_state=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# configure the cross-validation procedure\n",
    "cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# define the model\n",
    "model = LogisticRegression(random_state=1)\n",
    "# define search space\n",
    "space = dict()\n",
    "space['C'] = [0.1, 1, 10, 100]\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=1, cv=cv_inner, refit=True)\n",
    "# configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# execute the nested cross-validation\n",
    "scores = cross_val_score(search, Xs, clf_y, scoring='accuracy', cv=cv_outer, n_jobs=-1)\n",
    "# fit the search on the whole dataset to get the best model\n",
    "search.fit(Xs, clf_y)\n",
    "best_model = search.best_estimator_\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "print('Best Model:', best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MLE/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MLE/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MLE/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MLE/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy for PCR (cross-validated): 0.6094590053763441 Â± 0.021335278216603762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MLE/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_pCR = MLPClassifier(random_state=1, max_iter=1000)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "cv_scores_pCR = cross_val_score(mlp_pCR, Xs, clf_y, cv=kfold, scoring='balanced_accuracy')\n",
    "print(f'Balanced Accuracy for PCR (cross-validated): {cv_scores_pCR.mean()} Â± {cv_scores_pCR.std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MLE/lib/python3.10/site-packages/numpy/ma/core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "/opt/anaconda3/envs/MLE/lib/python3.10/site-packages/numpy/ma/core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.795 (0.017)\n",
      "Best Model: KNeighborsClassifier(metric='euclidean')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# configure the cross-validation procedure\n",
    "cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# define the model\n",
    "model = KNeighborsClassifier()\n",
    "# define search space\n",
    "space = dict()\n",
    "space['n_neighbors'] = [1, 5, 10, 15, 20]\n",
    "space['metric'] = ['euclidean', 'manhattan', 'minkowski']\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=1, cv=cv_inner, refit=True)\n",
    "# configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# execute the nested cross-validation\n",
    "scores = cross_val_score(search, Xs, clf_y, scoring='accuracy', cv=cv_outer, n_jobs=-1)\n",
    "# fit the search on the whole dataset to get the best model\n",
    "search.fit(Xs, clf_y)\n",
    "best_model = search.best_estimator_\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "print('Best Model:', best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "#joblib.dump({\"model\": model, \"scaler\": scaler, \"lda\": lda}, 'pcr_classification_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
