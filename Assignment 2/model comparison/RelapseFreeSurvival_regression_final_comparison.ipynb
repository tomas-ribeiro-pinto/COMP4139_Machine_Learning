{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of RFS Regression - Manifold vs PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold Learning with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before impute:\n",
      "Gene\n",
      "0      193\n",
      "1      119\n",
      "999     88\n",
      "Name: count, dtype: int64\n",
      "after impute:\n",
      "Gene\n",
      "0    281\n",
      "1    119\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/Library/Caches/pypoetry/virtualenvs/labs-env-NHQiDXzZ-py3.12/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400, 120)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all_df = pd.read_excel('../TrainDataset2024.xls', index_col=False)\n",
    "all_df.drop('ID', axis=1, inplace=True)\n",
    "all_df.shape\n",
    "\n",
    "import pickle\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import json\n",
    "keep_feat_names = []\n",
    "with open('../gene_clf_selected_features.json', 'rb') as f:\n",
    "  keep_feat_names = json.load(f)\n",
    "  \n",
    "# replace missing gene with classification result\n",
    "# see train_gene_classifier.ipynb for more details\n",
    "with open('../log_reg_gene_classifier.pkl', 'rb') as f:\n",
    "  log_res_clf = pickle.load(f)\n",
    "  \n",
    "  # rebuild prediction df\n",
    "  gene_impute_df = all_df.copy()\n",
    "\n",
    "  temp_X = gene_impute_df.drop(['pCR (outcome)', 'RelapseFreeSurvival (outcome)'], axis=1)\n",
    "  y = gene_impute_df['Gene']\n",
    "\n",
    "  print(\"before impute:\") \n",
    "  print(gene_impute_df['Gene'].value_counts())\n",
    "\n",
    "  keep_df = temp_X[keep_feat_names]\n",
    "  replace_index = keep_df[keep_df['Gene'] == 999].index\n",
    "\n",
    "  # get prediction on missing gene\n",
    "  target = gene_impute_df.loc[replace_index, keep_feat_names]\n",
    "  target.drop('Gene', axis=1, inplace=True)\n",
    "\n",
    "  pred = log_res_clf.predict(target)\n",
    "  gene_impute_df.loc[replace_index, 'Gene'] = pred\n",
    "\n",
    "  print(\"after impute:\") \n",
    "  print(gene_impute_df['Gene'].value_counts())\n",
    "\n",
    "  # assign back to all_df\n",
    "  all_df['Gene'] = gene_impute_df['Gene']\n",
    "\n",
    "# Replace missing values with median of the column\n",
    "imputer = SimpleImputer(strategy=\"median\", missing_values=999)\n",
    "all_df[:] = imputer.fit_transform(all_df)\n",
    "\n",
    "# classification target\n",
    "clf_y = all_df['pCR (outcome)']\n",
    "# regression target\n",
    "rgr_y = all_df['RelapseFreeSurvival (outcome)']\n",
    "\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier removal approach by:\n",
    "# Thanaki, Jalaj. Machine Learning Solutions : Expert Techniques to Tackle Complex Machine Learning Problems Using Python, Packt Publishing, Limited, 2018. \n",
    "# ProQuest Ebook Central, Available at: http://ebookcentral.proquest.com/lib/nottingham/detail.action?docID=5379696.\n",
    "\n",
    "# Outlier detection using the following methods:\n",
    "# 1. Percentile based outlier detection\n",
    "# 2. MAD (median absolute deviation) based outlier detection\n",
    "# 3. Standard deviation based outlier detection\n",
    "\n",
    "\"\"\" \n",
    "    Get all the data points that lie under the percentile range from 2.5 to 97.5\n",
    "\"\"\"\n",
    "def percentile_based_outlier(data, threshold=95):\n",
    "    diff = (100 - threshold) / 2.0\n",
    "    minval, maxval = np.percentile(data, [diff, 100 - diff])\n",
    "    return (data < minval) | (data > maxval)\n",
    "\n",
    "\"\"\"\n",
    "    Get all the data points that lie under a threshold of 3.5 using modified Z-score (based on the median absolute deviation)\n",
    "\"\"\"\n",
    "def mad_based_outlier(points, threshold=3.5):\n",
    "    points = np.array(points)\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:, None]\n",
    "    median_y = np.median(points)\n",
    "    median_absolute_deviation_y = np.median([np.abs(y - median_y) for y in points])\n",
    "    # Small constant added to avoid division by zero\n",
    "    modified_z_scores = [0.6745 * (y - median_y) / (median_absolute_deviation_y + 1e-6) for y in points]\n",
    "\n",
    "    return np.abs(modified_z_scores) > threshold\n",
    "\n",
    "\"\"\"\n",
    "    Get all the data points that lie under a threshold of 3 using standard deviation\n",
    "\"\"\"\n",
    "def std_div(data, threshold=3):\n",
    "    std = data.std()\n",
    "    mean = data.mean()\n",
    "    isOutlier = []\n",
    "    for val in data:\n",
    "        if abs(val - mean)/std > threshold:\n",
    "            isOutlier.append(True)\n",
    "        else:\n",
    "            isOutlier.append(False)\n",
    "    return isOutlier\n",
    "\n",
    "\"\"\"\n",
    "    Perform an outlier voting system to determine if a data point is an outlier. \n",
    "    If two of the three methods agree that a data point is an outlier, then it is removed.\n",
    "\"\"\"\n",
    "def outlierVote(data):\n",
    "    x = percentile_based_outlier(data)\n",
    "    y = mad_based_outlier(data)\n",
    "    z = std_div(data)\n",
    "    temp = list(zip(x, y, z))\n",
    "    final = []\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i].count(False) >= 2:\n",
    "            final.append(False)\n",
    "        else:\n",
    "            final.append(True)\n",
    "    return final\n",
    "\n",
    "def removeOutliers(data):\n",
    "    # Remove outliers from the dataframe\n",
    "    for column in data.columns:\n",
    "        outliers = outlierVote(all_df[column])\n",
    "        # Calculate Non-Outlier Maximum and minimum using the outliers list\n",
    "        non_outlier_max = all_df.loc[~np.array(outliers), column].max()\n",
    "        non_outlier_min = all_df.loc[~np.array(outliers), column].min()\n",
    "\n",
    "        # Replace outliers with the maximum or minimum non-outlier value\n",
    "        for i, outlier in enumerate(outliers):\n",
    "            if outlier:\n",
    "                data.loc[i, column] = non_outlier_max if data.loc[i, column] > non_outlier_max else non_outlier_min\n",
    "\n",
    "# Remove outliers, assign modified features to X and drop the outcome columns\n",
    "removeOutliers(all_df)\n",
    "X = all_df.drop(['pCR (outcome)', 'RelapseFreeSurvival (outcome)'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/Library/Caches/pypoetry/virtualenvs/labs-env-NHQiDXzZ-py3.12/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "[54.43768826 52.34407289 54.43788576 54.43851049 54.43794485 54.49990334\n",
      " 54.44435872 53.84006723 54.45210805 54.44891427]\n",
      "MAE:  21.4910116113379\n",
      "R2:  -0.017211227922942385\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xs_non_mri = scaler.fit_transform(X.iloc[:, :11])\n",
    "\n",
    "umap = UMAP(n_components=2, random_state=42)\n",
    "X_umap_mri = umap.fit_transform(X.iloc[:, 11:])\n",
    "\n",
    "Xs = np.c_[Xs_non_mri, X_umap_mri]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne_mri = tsne.fit_transform(X.iloc[:,11:])\n",
    "standard_scaler = StandardScaler()\n",
    "Xs_non_mri = standard_scaler.fit_transform(X.iloc[:,0:11])\n",
    "Xs = np.c_[Xs_non_mri, X_tsne_mri]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, rgr_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# svr \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 30, 45, 50, 75, 80, 85, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVR(), param_grid, n_jobs=-1, cv=5, verbose=1, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# validation \n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "best = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best.predict(X_test)\n",
    "print(y_pred[:10])\n",
    "\n",
    "print(\"MAE: \", mean_absolute_error(y_test, y_pred))\n",
    "print(\"R2: \", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - PCA on normally distributed MRI features + top 15 Best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Impute:\n",
      "28     999\n",
      "123    999\n",
      "161    999\n",
      "Name: Gene, dtype: int64\n",
      "After Impute:\n",
      "28     0\n",
      "123    0\n",
      "161    0\n",
      "Name: Gene, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/Library/Caches/pypoetry/virtualenvs/labs-env-NHQiDXzZ-py3.12/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandatory features: Index(['ER', 'HER2', 'Gene'], dtype='object')\n",
      "Normal MRI features: ['original_firstorder_Skewness', 'original_gldm_DependenceEntropy', 'original_glrlm_RunEntropy']\n",
      "Normal MRI PCA shape: (400, 1)\n",
      "Important features: ['original_firstorder_Range', 'original_firstorder_Kurtosis', 'original_glszm_ZoneEntropy', 'original_firstorder_90Percentile', 'Age', 'original_glszm_SizeZoneNonUniformity', 'original_firstorder_Skewness', 'original_firstorder_Maximum', 'original_glszm_SizeZoneNonUniformityNormalized', 'original_glszm_ZonePercentage', 'original_shape_Elongation', 'original_firstorder_Variance', 'original_shape_MinorAxisLength', 'original_shape_Flatness', 'original_glszm_SmallAreaHighGrayLevelEmphasis']\n",
      "Selected final features: 18 -  ['ER', 'HER2', 'Gene', 'original_firstorder_Range', 'original_firstorder_Kurtosis', 'original_glszm_ZoneEntropy', 'original_firstorder_90Percentile', 'Age', 'original_glszm_SizeZoneNonUniformity', 'original_firstorder_Skewness', 'original_firstorder_Maximum', 'original_glszm_SizeZoneNonUniformityNormalized', 'original_glszm_ZonePercentage', 'original_shape_Elongation', 'original_firstorder_Variance', 'original_shape_MinorAxisLength', 'original_shape_Flatness', 'original_glszm_SmallAreaHighGrayLevelEmphasis']\n",
      "Final Shape: (400, 19)\n",
      "Fitting 5 folds for each of 252 candidates, totalling 1260 fits\n",
      "Best parameters: {'max_depth': 4, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "MAE:  21.0759235495318\n",
      "R2:  0.027252923135139828\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all_df = pd.read_excel('../TrainDataset2024.xls', index_col=False)\n",
    "all_df.drop('ID', axis=1, inplace=True)\n",
    "all_df.head()\n",
    "\n",
    "'''\n",
    "Strategy: \n",
    "We trained a logistic regression model to predict the missing gene values.\n",
    "see the training for this model in train_gene_classifier.ipynb\n",
    "\n",
    "steps:\n",
    "1. Imputer missing Gene with gene_classifier predictions\n",
    "2. Impute missing values in all other columns with median \n",
    "'''\n",
    "\n",
    "import pickle\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "'''\n",
    "Load the features that were used in the training of the gene classifier.\n",
    "\n",
    "The gene_clf_selected_features.json file is created in train_gene_classifier.ipynb\n",
    "'''\n",
    "import json\n",
    "keep_feat_names = []\n",
    "with open('../gene_clf_selected_features.json', 'rb') as f:\n",
    "  keep_feat_names = json.load(f)\n",
    "  \n",
    "# Step 1. Imputer missing Gene with gene_classifier predictions\n",
    "# load the gene classifier model\n",
    "with open('../log_reg_gene_classifier.pkl', 'rb') as f:\n",
    "  log_res_clf = pickle.load(f)\n",
    "  \n",
    "  # Get row indicies of missing gene values\n",
    "  gene_impute_df = all_df.copy()\n",
    "  temp_X = gene_impute_df.drop(['pCR (outcome)', 'RelapseFreeSurvival (outcome)'], axis=1)\n",
    "  y = gene_impute_df['Gene']\n",
    "  keep_df = temp_X[keep_feat_names]\n",
    "  replace_index = keep_df[keep_df['Gene'] == 999].index\n",
    "\n",
    "  print(\"Before Impute:\")\n",
    "  print(gene_impute_df.iloc[replace_index, :]['Gene'][:3])\n",
    "\n",
    "  # shape the target data set\n",
    "  target = gene_impute_df.loc[replace_index, keep_feat_names]\n",
    "  target.drop('Gene', axis=1, inplace=True)\n",
    "\n",
    "  # make classification predictions\n",
    "  pred = log_res_clf.predict(target)\n",
    "  # replace missing gene values with predictions\n",
    "  gene_impute_df.loc[replace_index, 'Gene'] = pred\n",
    "\n",
    "  print(\"After Impute:\") \n",
    "  print(gene_impute_df.iloc[replace_index, :]['Gene'][:3])\n",
    "\n",
    "  # assign the gene values back to the main dataframe\n",
    "  all_df['Gene'] = gene_impute_df['Gene']\n",
    "\n",
    "# Step 2. Impute missing values in all other columns with median\n",
    "# Replace missing values with median of the column\n",
    "imputer = SimpleImputer(strategy=\"median\", missing_values=999)\n",
    "all_df[:] = imputer.fit_transform(all_df)\n",
    "\n",
    "# Save regression target label for later use\n",
    "y = all_df['RelapseFreeSurvival (outcome)']\n",
    "\n",
    "removeOutliers(all_df)\n",
    "X = all_df.drop(['pCR (outcome)', 'RelapseFreeSurvival (outcome)'], axis=1)\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "'''\n",
    "  Feature Selection and Dimensionality Reduction strategy:\n",
    "  Combine the following feature categories:\n",
    "    1. Mandatory features - 'ER', 'HER2', 'Gene'\n",
    "    2. Select the nomral MRI features and apply PCA\n",
    "    3. Select the best features from the rest of the features (non MRI)\n",
    "  Then combine all the selected features and apply PCA to reduce the dimensionality of the final feature set.\n",
    "'''\n",
    "\n",
    "X = all_df.drop(['pCR (outcome)', 'RelapseFreeSurvival (outcome)'], axis=1)\n",
    "\n",
    "# A dictionary for storing columns names used during training for the final test predictions\n",
    "cache = {} \n",
    "\n",
    "# 1. Mandarory feartures\n",
    "mandatory_features_indices = [1,3,10]\n",
    "print(f\"Mandatory features: {X.columns[mandatory_features_indices]}\")\n",
    "\n",
    "# 2. Normally distributed MRI features and apply PCA\n",
    "mri_indices = list(range(11, X.shape[1]))\n",
    "mri_features = X.iloc[:, mri_indices]\n",
    "alpha = 0.05\n",
    "normal_cols_idx = []\n",
    "for idx, col in enumerate(mri_features.columns):\n",
    "  res = stats.normaltest(mri_features[col])\n",
    "  if res.pvalue > alpha:\n",
    "    normal_cols_idx.append(idx)\n",
    "print(f\"Normal MRI features: {list(mri_features.columns[normal_cols_idx])}\")\n",
    "\n",
    "# Apply PCA to the MRI features\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "X_mri = X.iloc[:, normal_cols_idx]\n",
    "X_mri_pca = pca.fit_transform(X_mri)\n",
    "print(f\"Normal MRI PCA shape: {X_mri_pca.shape}\")\n",
    "\n",
    "# save normally distributed MRI features index\n",
    "cache['normal_cols_idx'] = list(normal_cols_idx)\n",
    "\n",
    "# 3. Select best features from the rest of the features\n",
    "# Start by selectinf best features from the all features\n",
    "reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "reg.fit(X, y)\n",
    "importances = reg.feature_importances_\n",
    "important_features_idx = np.argsort(importances)[::-1][:15]\n",
    "\n",
    "# Filter out normally distributed MRI features\n",
    "important_features_idx = [i for i in important_features_idx if i not in normal_cols_idx and i not in mandatory_features_indices]\n",
    "print(f\"Important features: {list(X.columns[important_features_idx])}\")\n",
    "cache['important_features_idx'] = [int(idx) for idx in important_features_idx]\n",
    "\n",
    "# 4. Build final feature set\n",
    "# Combine Mandatory and important features indices\n",
    "final_indices = mandatory_features_indices + important_features_idx\n",
    "print(f\"Selected final features: {len(final_indices)} -  {list(X.columns[final_indices])}\")\n",
    "\n",
    "# Add pca of normal MRI features\n",
    "Xs = X.iloc[:, final_indices]\n",
    "Xs = np.concatenate((Xs, X_mri_pca), axis=1)\n",
    "print(f\"Final Shape: {Xs.shape}\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error\n",
    "rnd_forest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "'''\n",
    "Use GridSearchCV to find the best hyperparameters for the model\n",
    "'''\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 4, 7, 11, 50, 100],\n",
    "    'n_estimators': [50, 75, 100],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_split': [2, 5, 7, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rnd_forest, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_rnd_forest = grid_search.best_estimator_\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = best_rnd_forest.predict(X_test)\n",
    "rnd_mae = mean_absolute_error(y_test, y_pred)\n",
    "rnd_rmse = root_mean_squared_error(y_test, y_pred)\n",
    "rnd_r2 = r2_score(y_test, y_pred)\n",
    "result = {\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'mean_absolute_error': rnd_mae,\n",
    "    'root_mean_squared_error': rnd_rmse,\n",
    "    'r2_score': rnd_r2\n",
    "}\n",
    "print(\"MAE: \", rnd_mae)\n",
    "print(\"R2: \", rnd_r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs-env-NHQiDXzZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
